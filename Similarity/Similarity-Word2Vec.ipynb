{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "667ce5e3-c76f-4700-b0f1-befc2b56ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mcc\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47088fa-2a66-4de3-a488-97ce5728dda5",
   "metadata": {},
   "source": [
    "# Training Word2vec (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "488c20db-9eca-4429-9863-0aa3686ac53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "01916eac-e194-4be0-a693-bd9a439032c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>Country</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>IsReshare</th>\n",
       "      <th>Reach</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Klout</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tw-1267804344</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Elbasan</td>\n",
       "      <td>Elbasan</td>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "      <td>tw-698155297102295041</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>Friday</td>\n",
       "      <td>True</td>\n",
       "      <td>339.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>\"RT @AdrianRusso82: Our Innovation Lab is offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tw-27229880</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Tirane</td>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "      <td>tw-685159757209059329</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>False</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Now Open  AWS Asia Pacific (Seoul) Region via ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tw-27229880</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Tirane</td>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "      <td>tw-686907710311378944</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>A Beginner's Guide to Scaling to 11 Million+ U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tw-27229880</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Tirane</td>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "      <td>tw-686968158050201600</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Bridging AWS and Azure environments via VPN vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tw-27229880</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Tirane</td>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "      <td>tw-690210449674092545</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>False</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>ELK on AWS ElasticSearch Service + ElasticBean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         UserID   Gender  LocationID     City    State StateCode  \\\n",
       "0           0  tw-1267804344  Unknown         1.0  Elbasan  Elbasan        AL   \n",
       "1           1    tw-27229880     Male         2.0   Tirana   Tirane        AL   \n",
       "2           2    tw-27229880     Male         2.0   Tirana   Tirane        AL   \n",
       "3           3    tw-27229880     Male         2.0   Tirana   Tirane        AL   \n",
       "4           4    tw-27229880     Male         2.0   Tirana   Tirane        AL   \n",
       "\n",
       "   Country                TweetID  Hour  Day   Weekday  IsReshare  Reach  \\\n",
       "0  Albania  tw-698155297102295041     7   12    Friday       True  339.0   \n",
       "1  Albania  tw-685159757209059329    11    7  Thursday      False   87.0   \n",
       "2  Albania  tw-686907710311378944     6   12   Tuesday      False   87.0   \n",
       "3  Albania  tw-686968158050201600    10   12   Tuesday      False   87.0   \n",
       "4  Albania  tw-690210449674092545     9   21  Thursday      False   85.0   \n",
       "\n",
       "   RetweetCount  Likes  Klout  Sentiment Lang  \\\n",
       "0         127.0    0.0   44.0        0.0   en   \n",
       "1           0.0    0.0   22.0        0.0   en   \n",
       "2           0.0    0.0   22.0        0.0   en   \n",
       "3           0.0    0.0   22.0        0.0   en   \n",
       "4           0.0    0.0   21.0        0.0   en   \n",
       "\n",
       "                                                text  \n",
       "0  \"RT @AdrianRusso82: Our Innovation Lab is offi...  \n",
       "1  Now Open  AWS Asia Pacific (Seoul) Region via ...  \n",
       "2  A Beginner's Guide to Scaling to 11 Million+ U...  \n",
       "3  Bridging AWS and Azure environments via VPN vi...  \n",
       "4  ELK on AWS ElasticSearch Service + ElasticBean...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets-engagement-metrics.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ab248116-1528-45d1-ab1a-85b063f3bf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102062, 20)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a4d3e9fc-a3d2-4473-86c8-7c718eb9fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = df.text.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a876e8d4-9514-4e0d-b4eb-21007d3ea4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @pzerger: Microsoft and Google Lead in Cloud Growth AWS Rules Overall http://www.datamation.com/cloud-computing/microsoft-and-google-lead-in-cloud-growth-aws-rules-overall.html #Azure'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "031fb922-6ddc-41f7-b2b3-0f4d4d673095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'pzerger',\n",
       " 'microsoft',\n",
       " 'and',\n",
       " 'google',\n",
       " 'lead',\n",
       " 'in',\n",
       " 'cloud',\n",
       " 'growth',\n",
       " 'aws',\n",
       " 'rules',\n",
       " 'overall',\n",
       " 'http',\n",
       " 'www',\n",
       " 'datamation',\n",
       " 'com',\n",
       " 'cloud',\n",
       " 'computing',\n",
       " 'microsoft',\n",
       " 'and',\n",
       " 'google',\n",
       " 'lead',\n",
       " 'in',\n",
       " 'cloud',\n",
       " 'growth',\n",
       " 'aws',\n",
       " 'rules',\n",
       " 'overall',\n",
       " 'html',\n",
       " 'azure']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text.loc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9ea530fd-73d0-4e10-ab39-353235d9f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=40,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "02d64548-aff3-4153-b458-b3a02fb49c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(review_text, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "06cc0334-a5d7-47be-a4f8-38ab52b5fe74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10028791, 13474175)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d29e7309-2ecc-4205-bb7f-cde06bbeb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./Tweets.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "01b400aa-735f-4ca4-af19-90ed10cb27ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69041467"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"great\", w2=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ba7782e6-bea8-49c1-b16e-7dd9cfe4232a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yrprkv', 0.8794959783554077),\n",
       " ('addresses', 0.8272523880004883),\n",
       " ('ip', 0.7782436609268188),\n",
       " ('requests', 0.7722018361091614),\n",
       " ('generating', 0.7255535125732422),\n",
       " ('block', 0.7250475883483887),\n",
       " ('generate', 0.6471606492996216),\n",
       " ('input', 0.5709637403488159),\n",
       " ('compression', 0.5544778108596802),\n",
       " ('deleted', 0.5477204322814941)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0e4fcc92-22ec-45f0-91ae-4d630bad2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \" how many seats are on the next flight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "631e1490-b000-4be2-8e00-37ab6d362e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"No warnings or communication until we were 15 minutes Late Flight.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a3aecf4f-7018-4140-8122-7fcbde448ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = pd.Series(text1).apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0bdff2be-1819-46dc-929d-df29af9abb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = pd.Series(text2).apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "44466e7f-4768-4d7d-a191-c30679c5632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'warnings',\n",
       " 'or',\n",
       " 'communication',\n",
       " 'until',\n",
       " 'we',\n",
       " 'were',\n",
       " 'minutes',\n",
       " 'late',\n",
       " 'flight']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "79d61c94-23f0-420c-adf2-704e9b898899",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Artificial intelligence (AI) is advancing at a rapid pace, revolutionizing various industries such as healthcare, finance, and transportation. Machine learning algorithms are enabling computers to perform tasks that previously required human intelligence, from diagnosing diseases to driving cars autonomously. The potential benefits of AI are immense, including improved efficiency, increased accuracy, and enhanced decision-making capabilities. However, there are also concerns about the ethical implications and potential risks associated with AI, such as job displacement, bias in algorithms, and loss of privacy. As AI continues to evolve, it is crucial for policymakers, researchers, and society as a whole to address these challenges and ensure that AI technologies are developed and deployed responsibly.\"\n",
    "text2 = \"The history of ancient civilizations is a fascinating subject that has captivated scholars and historians for centuries. From the majestic pyramids of Egypt to the intricate ruins of Machu Picchu, each civilization has left behind a legacy of art, architecture, and culture that continues to inspire awe and wonder today. By studying ancient texts, artifacts, and archaeological findings, researchers have been able to piece together the stories of these ancient societies and gain insights into their beliefs, customs, and daily lives. However, many aspects of ancient civilizations remain shrouded in mystery, leaving room for speculation and debate among experts. Despite the challenges of deciphering ancient languages and interpreting ancient sources, the study of ancient civilizations remains a vibrant and dynamic field of inquiry, offering valuable lessons about the human experience and our place in the world.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "440b776a-faae-4a01-933d-2f4b94e88832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two sentences: 0.8347887\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained word2vec model\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('path_to_word2vec.bin', binary=True)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    # Tokenize the sentence\n",
    "    tokens = sentence.split()\n",
    "    # Initialize an array to store word vectors\n",
    "    word_vectors_list = []\n",
    "    # Iterate over tokens and get word vectors\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            word_vectors_list.append(model.wv[token])\n",
    "    # If no valid word vectors found, return None\n",
    "    if not word_vectors_list:\n",
    "        return None\n",
    "    # Calculate the average of word vectors\n",
    "    return np.mean(word_vectors_list, axis=0)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute cosine similarity between two vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"the like dogs\"\n",
    "sentence2 = \"Im Like dogs\"\n",
    "\n",
    "# Example word2vec model\n",
    "# model = Word2Vec.load('path_to_word2vec_model')\n",
    "\n",
    "# Get sentence vectors\n",
    "vec1 = get_sentence_vector(text1, model)\n",
    "vec2 = get_sentence_vector(text2, model)\n",
    "\n",
    "if vec1 is not None and vec2 is not None:\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    print(\"Similarity between the two sentences:\", similarity)\n",
    "else:\n",
    "    print(\"Unable to compute similarity. Make sure the sentences contain valid words in the word2vec model vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e90082-5fd6-4730-97e4-d05d6f39e3b4",
   "metadata": {},
   "source": [
    "# Pretrained Word2vec \"GoogleNews\" (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "79b0f472-0848-481e-9998-6d1f83ccf5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MCC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f0b9fd35-c0b9-4032-96ad-e906dcb1317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.9780031\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "# Load pre-trained Word2Vec model\n",
    "model_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"This is the first text.\"\n",
    "text2 = \"This is the second text.\"\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "tokens1 = word_tokenize(text1.lower())\n",
    "tokens2 = word_tokenize(text2.lower())\n",
    "\n",
    "# Compute text embeddings\n",
    "def text_embedding(tokens):\n",
    "    embeddings = [word_vectors.get_vector(word) for word in tokens if word in word_vectors.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Return zero vector if no words found in vocabulary\n",
    "\n",
    "embedding1 = text_embedding(tokens1)\n",
    "embedding2 = text_embedding(tokens2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "035d4b21-f53b-48e1-95de-a847ad4a5375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.6747796\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Artificial intelligence (AI) is advancing at a rapid pace, revolutionizing various industries such as healthcare, finance, and transportation. Machine learning algorithms are enabling computers to perform tasks that previously required human intelligence, from diagnosing diseases to driving cars autonomously. The potential benefits of AI are immense, including improved efficiency, increased accuracy, and enhanced decision-making capabilities. However, there are also concerns about the ethical implications and potential risks associated with AI, such as job displacement, bias in algorithms, and loss of privacy. As AI continues to evolve, it is crucial for policymakers, researchers, and society as a whole to address these challenges and ensure that AI technologies are developed and deployed responsibly.\"\n",
    "text2 = \"The history of ancient civilizations is a fascinating subject that has captivated scholars and historians for centuries. From the majestic pyramids of Egypt to the intricate ruins of Machu Picchu, each civilization has left behind a legacy of art, architecture, and culture that continues to inspire awe and wonder today. By studying ancient texts, artifacts, and archaeological findings, researchers have been able to piece together the stories of these ancient societies and gain insights into their beliefs, customs, and daily lives. However, many aspects of ancient civilizations remain shrouded in mystery, leaving room for speculation and debate among experts. Despite the challenges of deciphering ancient languages and interpreting ancient sources, the study of ancient civilizations remains a vibrant and dynamic field of inquiry, offering valuable lessons about the human experience and our place in the world.\"\n",
    "# Tokenization and preprocessing\n",
    "tokens1 = word_tokenize(text1.lower())\n",
    "tokens2 = word_tokenize(text2.lower())\n",
    "\n",
    "# Compute text embeddings\n",
    "def text_embedding(tokens):\n",
    "    embeddings = [word_vectors.get_vector(word) for word in tokens if word in word_vectors.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Return zero vector if no words found in vocabulary\n",
    "\n",
    "embedding1 = text_embedding(tokens1)\n",
    "embedding2 = text_embedding(tokens2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0d2b1aff-54e4-40e4-a854-9ed77260d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.8934646\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "\"The sun rises in the east, casting its warm rays upon the earth. Birds chirp merrily as they greet the dawn, \n",
    "while the gentle breeze rustles through the leaves of the trees. It's a new day, full of promise and possibility.\"\n",
    "\"\"\"\n",
    "\n",
    "text2 =  \"\"\"\n",
    "\"As the day begins, the sun ascends from the eastern horizon, illuminating the world with its radiant light. \n",
    "The melodious songs of birds fill the air, accompanied by the soft whispers of the wind among the branches. \n",
    "It's a fresh beginning, brimming with hope and opportunity.\"\n",
    "\"\"\"\n",
    "# Tokenization and preprocessing\n",
    "tokens1 = word_tokenize(text1.lower())\n",
    "tokens2 = word_tokenize(text2.lower())\n",
    "\n",
    "# Compute text embeddings\n",
    "def text_embedding(tokens):\n",
    "    embeddings = [word_vectors.get_vector(word) for word in tokens if word in word_vectors.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Return zero vector if no words found in vocabulary\n",
    "\n",
    "embedding1 = text_embedding(tokens1)\n",
    "embedding2 = text_embedding(tokens2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2fbd5729-da07-4652-8edb-df7909046624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.8929715\n"
     ]
    }
   ],
   "source": [
    "text1 =  \"\"\"\n",
    "\"The ocean waves crash against the rugged cliffs, sending salty spray into the air. Seagulls soar overhead, \n",
    "their cries echoing in the vast expanse of the sky. The scent of salt and seaweed permeates the air, a reminder \n",
    "of the eternal dance between land and sea.\"\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "\"On the coast, waves collide with the sturdy rocks, splashing droplets high into the air. Above, seagulls glide \n",
    "gracefully, their calls mingling with the sounds of the crashing waves. The briny aroma of the sea fills the \n",
    "atmosphere, a testament to the perpetual interplay of land and water.\"\n",
    "\"\"\"\n",
    "# Tokenization and preprocessing\n",
    "tokens1 = word_tokenize(text1.lower())\n",
    "tokens2 = word_tokenize(text2.lower())\n",
    "\n",
    "# Compute text embeddings\n",
    "def text_embedding(tokens):\n",
    "    embeddings = [word_vectors.get_vector(word) for word in tokens if word in word_vectors.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Return zero vector if no words found in vocabulary\n",
    "\n",
    "embedding1 = text_embedding(tokens1)\n",
    "embedding2 = text_embedding(tokens2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c3f6e637-b288-45ca-8285-f00ec4dda128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.7603578\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "\"The city streets bustle with activity as people hurry to and fro, lost in their own thoughts and concerns. \n",
    "Car horns blare in a cacophony of sound, competing with the chatter of pedestrians and the hum of engines. \n",
    "Amidst the hustle and bustle, life carries on unabated.\"\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "\"In the quiet of the countryside, fields stretch out as far as the eye can see, bathed in the soft glow of \n",
    "the setting sun. The only sounds are the gentle rustle of the wind through the grass and the occasional chirp \n",
    "of a cricket. It's a tranquil scene, far removed from the chaos of the city.\"\n",
    "\"\"\"\n",
    "# Tokenization and preprocessing\n",
    "tokens1 = word_tokenize(text1.lower())\n",
    "tokens2 = word_tokenize(text2.lower())\n",
    "\n",
    "# Compute text embeddings\n",
    "def text_embedding(tokens):\n",
    "    embeddings = [word_vectors.get_vector(word) for word in tokens if word in word_vectors.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Return zero vector if no words found in vocabulary\n",
    "\n",
    "embedding1 = text_embedding(tokens1)\n",
    "embedding2 = text_embedding(tokens2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e463bcbb-c43c-44d0-8abd-fa4646aed278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.5747914\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Artificial intelligence (AI) is advancing at a rapid pace, revolutionizing various industries such as healthcare, finance, and transportation. Machine learning algorithms are enabling computers to perform tasks that previously required human intelligence, from diagnosing diseases to driving cars autonomously. The potential benefits of AI are immense, including improved efficiency, increased accuracy, and enhanced decision-making capabilities. However, there are also concerns about the ethical implications and potential risks associated with AI, such as job displacement, bias in algorithms, and loss of privacy. As AI continues to evolve, it is crucial for policymakers, researchers, and society as a whole to address these challenges and ensure that AI technologies are developed and deployed responsibly.\"\n",
    "text2 = \"\"\"\n",
    "\"In the quiet of the countryside, fields stretch out as far as the eye can see, bathed in the soft glow of \n",
    "the setting sun. The only sounds are the gentle rustle of the wind through the grass and the occasional chirp \n",
    "of a cricket. It's a tranquil scene, far removed from the chaos of the city.\"\n",
    "\"\"\"\n",
    "# Tokenization and preprocessing\n",
    "tokens1 = word_tokenize(text1.lower())\n",
    "tokens2 = word_tokenize(text2.lower())\n",
    "\n",
    "# Compute text embeddings\n",
    "def text_embedding(tokens):\n",
    "    embeddings = [word_vectors.get_vector(word) for word in tokens if word in word_vectors.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Return zero vector if no words found in vocabulary\n",
    "\n",
    "embedding1 = text_embedding(tokens1)\n",
    "embedding2 = text_embedding(tokens2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fc114-b5b9-45e6-9f61-e1ec94a28280",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb91582-4b95-47ac-9fc2-70f179a132be",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "#Crypto\n",
    "'Investors unfazed by correction as crypto funds see $154 million inflows',\n",
    "'Bitcoin, Ethereum prices continue descent, but crypto funds see inflows',\n",
    " \n",
    "#Inflation\n",
    "'The surge in euro area inflation during the pandemic: transitory but with upside risks',\n",
    "\"Inflation: why it's temporary and raising interest rates will do more harm than good\",\n",
    " \n",
    "#common\n",
    "'Will Cryptocurrency Protect Against Inflation?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8050839-efde-462b-bdd0-588f0f085fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f2d266-2a0d-4896-a171-5ff87f75e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/42.8 MB 326.8 kB/s eta 0:02:11\n",
      "     --------------------------------------- 0.1/42.8 MB 521.8 kB/s eta 0:01:22\n",
      "     --------------------------------------- 0.2/42.8 MB 871.5 kB/s eta 0:00:49\n",
      "     ---------------------------------------- 0.3/42.8 MB 1.1 MB/s eta 0:00:39\n",
      "     ---------------------------------------- 0.5/42.8 MB 1.6 MB/s eta 0:00:27\n",
      "      --------------------------------------- 0.8/42.8 MB 2.2 MB/s eta 0:00:20\n",
      "     - -------------------------------------- 1.1/42.8 MB 2.8 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 2.0/42.8 MB 4.5 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 2.5/42.8 MB 5.1 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 3.2/42.8 MB 5.9 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.2/42.8 MB 7.2 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 4.9/42.8 MB 7.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 5.8/42.8 MB 8.7 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 6.8/42.8 MB 9.4 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 7.2/42.8 MB 9.8 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 7.2/42.8 MB 9.8 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 7.2/42.8 MB 9.8 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 7.3/42.8 MB 8.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 9.1/42.8 MB 9.5 MB/s eta 0:00:04\n",
      "     ---------- ---------------------------- 11.5/42.8 MB 18.2 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.2/42.8 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 13.1/42.8 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 14.2/42.8 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.9/42.8 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.9/42.8 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.9/42.8 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 15.1/42.8 MB 14.6 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 19.0/42.8 MB 24.2 MB/s eta 0:00:01\n",
      "     ----------------- --------------------- 19.7/42.8 MB 21.8 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 21.0/42.8 MB 20.5 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 22.4/42.8 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 23.8/42.8 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 25.2/42.8 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 26.6/42.8 MB 32.7 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 28.1/42.8 MB 28.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 29.6/42.8 MB 29.8 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 30.7/42.8 MB 31.1 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 32.6/42.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 33.7/42.8 MB 29.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 35.3/42.8 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 36.6/42.8 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 38.1/42.8 MB 29.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.5/42.8 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 40.4/42.8 MB 29.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 41.5/42.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.8/42.8 MB 23.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\mcc\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.0)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cb71d9-f186-4862-ac82-287d44b234f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6d3134-00f8-4db3-aadc-2e9cdb8607e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "docs = [nlp(headline) for headline in headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d85d1b89-0186-462b-ad33-36ecdc891653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_heatmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m       row\u001b[38;5;241m.\u001b[39mappend(docs[i]\u001b[38;5;241m.\u001b[39msimilarity(docs[j]))\n\u001b[0;32m      6\u001b[0m     similarity\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[1;32m----> 7\u001b[0m create_heatmap(similarity)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_heatmap' is not defined"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "for i in range(len(docs)):\n",
    "    row = []\n",
    "    for j in range(len(docs)):\n",
    "      row.append(docs[i].similarity(docs[j]))\n",
    "    similarity.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c136a84-3fbb-4cfe-a076-fd4ea17c4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.5029938457466879\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Preprocess texts\n",
    "    tokens1 = preprocess_text(text1)\n",
    "    tokens2 = preprocess_text(text2)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity_sum = 0\n",
    "    count = 0\n",
    "    for token1 in tokens1:\n",
    "        max_similarity = 0\n",
    "        for token2 in tokens2:\n",
    "            similarity = nlp(token1).similarity(nlp(token2))\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "        similarity_sum += max_similarity\n",
    "        count += 1\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    if count > 0:\n",
    "        avg_similarity = similarity_sum / count\n",
    "    else:\n",
    "        avg_similarity = 0\n",
    "    \n",
    "    return avg_similarity\n",
    "\n",
    "# Example texts\n",
    "text1 = \"The city bustled with life, streets filled with the symphony of honking horns and hurried footsteps. Neon lights illuminated the night, casting colorful reflections on wet pavements. Cafés buzzed with conversation, and the aroma of coffee lingered in the air.\"\n",
    "text2 = \"The moon casts its soft glow over the silent night. Stars twinkle in the darkness, like diamonds scattered across velvet. The world sleeps peacefully, wrapped in a blanket of tranquility. A calmness settles in, soothing the restless soul with its gentle embrace.\"\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "print(\"Similarity Score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89e59d-0f8d-4f6f-83a1-b2f9f3088f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
